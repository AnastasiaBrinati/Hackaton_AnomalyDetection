{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Coding: Testing in MLOps con Prometheus e Grafana\n",
    "\n",
    "**Obiettivo della lezione**: Imparare a monitorare un modello di Machine Learning in un ambiente di produzione simulato, utilizzando Prometheus per la raccolta di metriche e Grafana per la loro visualizzazione. Questo processo è una forma di **testing continuo** fondamentale nelle pratiche MLOps.\n",
    "\n",
    "**Dataset**: Useremo il [Credit Card Fraud Detection Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud). Questo dataset è ideale perché è sbilanciato e ci permette di monitorare metriche interessanti come il numero di frodi rilevate.\n",
    "\n",
    "**Prerequisiti software**:\n",
    "1.  **Docker e Docker Compose**: Per eseguire Prometheus e Grafana in container isolati.\n",
    "2.  **Python 3.8+**\n",
    "3.  Le librerie Python che installeremo tra poco.\n",
    "4.  Il file `creditcard.csv` scaricato e presente in questa cartella."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cella 1: Installazione delle librerie\n",
    "\n",
    "Iniziamo installando le librerie necessarie per questo progetto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn==1.3.2 flask prometheus-client requests joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cella 2: Setup dell'Ambiente con Docker\n",
    "\n",
    "Per eseguire Prometheus e Grafana, useremo Docker Compose. È il modo più semplice per gestire più container. \n",
    "\n",
    "**Passo 1: Creare i file di configurazione**\n",
    "\n",
    "Nella stessa cartella di questo notebook, crea una sotto-cartella chiamata `prometheus`. All'interno di `prometheus`, crea un file chiamato `prometheus.yml` con il seguente contenuto:\n",
    "\n",
    "```yaml\n",
    "# prometheus/prometheus.yml\n",
    "global:\n",
    "  scrape_interval: 15s # Frequenza con cui raccogliere le metriche\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'ml-model-app'\n",
    "    # L'host 'host.docker.internal' è una keyword speciale di Docker\n",
    "    # che permette al container di raggiungere i servizi in esecuzione sulla macchina host (il tuo PC)\n",
    "    static_configs:\n",
    "      - targets: ['host.docker.internal:5000'] \n",
    "```\n",
    "\n",
    "Successivamente, nella cartella principale (la stessa di questo notebook), crea un file chiamato `docker-compose.yml`:\n",
    "\n",
    "```yaml\n",
    "# docker-compose.yml\n",
    "version: '3.7'\n",
    "\n",
    "services:\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    container_name: prometheus\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus:/etc/prometheus\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    container_name: grafana\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    depends_on:\n",
    "      - prometheus\n",
    "```\n",
    "\n",
    "**Passo 2: Avviare i container**\n",
    "\n",
    "Apri un terminale nella cartella di questo notebook ed esegui:\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "Ora hai Prometheus in esecuzione su `http://localhost:9090` e Grafana su `http://localhost:3000`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cella 3: Addestramento del Modello di Machine Learning\n",
    "\n",
    "Ora prepariamo il nostro modello. Useremo `scikit-learn` per addestrare un `LogisticRegression` sul dataset delle frodi. Il modello addestrato e lo `scaler` (usato per normalizzare i dati) verranno salvati su disco per essere usati dalla nostra API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "print(\"Caricamento del dataset... (potrebbe richiedere qualche istante)\")\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "print(\"Preparazione dei dati...\")\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Normalizziamo i dati: è fondamentale per molti modelli, inclusa la Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Divisione in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Addestramento del modello...\")\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Valutazione del modello sul test set:\")\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Salvataggio del modello e dello scaler per l'uso nell'API\n",
    "joblib.dump(model, 'model.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "\n",
    "print(\"\\nModello e scaler salvati come 'model.joblib' e 'scaler.joblib'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cella 4: Creare un'API per il Modello con Flask\n",
    "\n",
    "Ora che abbiamo un modello, dobbiamo \"servirlo\" tramite un'API. Useremo **Flask**. La parte cruciale è l'**instrumentazione**: aggiungeremo codice per esporre le metriche che Prometheus potrà leggere.\n",
    "\n",
    "**Metriche che tracceremo**:\n",
    "1.  `predictions_total`: Contatore del numero totale di predizioni (con etichetta `fraud`/`non_fraud`).\n",
    "2.  `prediction_latency_seconds`: Istogramma per misurare il tempo di risposta.\n",
    "3.  `model_accuracy`: Un *gauge* (misuratore) che potremmo aggiornare per monitorare l'accuratezza.\n",
    "\n",
    "**Azione richiesta**: Salva il codice seguente in un file separato chiamato `app.py` nella stessa cartella di questo notebook.\n",
    "\n",
    "```python\n",
    "# app.py\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import time\n",
    "from flask import Flask, request, jsonify\n",
    "from prometheus_client import Counter, Histogram, make_wsgi_app\n",
    "\n",
    "# 1. Inizializza l'applicazione Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# 2. Carica il modello e lo scaler addestrati\n",
    "model = joblib.load('model.joblib')\n",
    "scaler = joblib.load('scaler.joblib')\n",
    "\n",
    "# 3. Definisci le metriche di Prometheus\n",
    "PREDICTIONS_TOTAL = Counter(\n",
    "    'predictions_total', # Nome della metrica\n",
    "    'Total number of predictions made',\n",
    "    ['class_name'] # Etichetta per distinguere le classi\n",
    ")\n",
    "PREDICTION_LATENCY = Histogram(\n",
    "    'prediction_latency_seconds',\n",
    "    'Time taken to process a prediction request'\n",
    ")\n",
    "\n",
    "# 4. Crea l'endpoint di predizione `/predict`\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Prendi i dati JSON dalla richiesta\n",
    "    data = request.get_json()\n",
    "    df = pd.DataFrame(data, index=)\n",
    "    \n",
    "    # Applica lo scaler (DEVE essere lo stesso usato in addestramento)\n",
    "    data_scaled = scaler.transform(df)\n",
    "    \n",
    "    # Esegui la predizione\n",
    "    prediction = model.predict(data_scaled)\n",
    "    class_name = \"fraud\" if prediction == 1 else \"non_fraud\"\n",
    "    \n",
    "    # Aggiorna il contatore delle predizioni\n",
    "    PREDICTIONS_TOTAL.labels(class_name=class_name).inc()\n",
    "        \n",
    "    # Calcola e registra la latenza\n",
    "    latency = time.time() - start_time\n",
    "    PREDICTION_LATENCY.observe(latency)\n",
    "    \n",
    "    return jsonify({'prediction': int(prediction), 'class_name': class_name})\n",
    "\n",
    "# 5. Esponi l'endpoint `/metrics` per Prometheus\n",
    "# La libreria prometheus_client si occupa di creare questo endpoint per noi\n",
    "from werkzeug.middleware.dispatcher import DispatcherMiddleware\n",
    "app.wsgi_app = DispatcherMiddleware(app.wsgi_app, {\n",
    "    '/metrics': make_wsgi_app()\n",
    "})\n",
    "\n",
    "# 6. Avvia l'applicazione\n",
    "if __name__ == '__main__':\n",
    "    # Esegui su porta 5000, accessibile da tutte le interfacce di rete ('0.0.0.0')\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "```\n",
    "\n",
    "**Passo successivo**: Apri un **nuovo terminale**, vai nella cartella del progetto ed esegui:\n",
    "```bash\n",
    "python app.py\n",
    "```\n",
    "Se tutto è corretto, il server Flask è in esecuzione. Puoi verificare che le metriche siano esposte visitando `http://localhost:5000/metrics` nel tuo browser (al momento saranno a zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cella 5: Visualizzazione con Grafana\n",
    "\n",
    "È il momento di visualizzare! Andremo sull'interfaccia di Grafana e costruiremo una dashboard.\n",
    "\n",
    "**Passo 1: Accedi a Grafana**\n",
    "- Apri `http://localhost:3000`.\n",
    "- **Username**: admin, **Password**: admin (ti chiederà di cambiarla).\n",
    "\n",
    "**Passo 2: Aggiungi Prometheus come Data Source**\n",
    "1.  Menu a sinistra (icona ingranaggio) -> **Data Sources** -> **Add data source**.\n",
    "2.  Seleziona **Prometheus**.\n",
    "3.  URL: `http://prometheus:9090`. (Usiamo `prometheus` come hostname perché i container sono nella stessa rete Docker).\n",
    "4.  Clicca **Save & Test**.\n",
    "\n",
    "**Passo 3: Crea una Dashboard**\n",
    "1.  Menu a sinistra (icona 4 quadrati) -> **Dashboards** -> **New Dashboard** -> **Add visualization**.\n",
    "\n",
    "**Pannello 1: Latenza delle Predizioni (95° percentile)**\n",
    "- **Query PromQL**: `histogram_quantile(0.95, sum(rate(prediction_latency_seconds_bucket[5m])) by (le))`\n",
    "- **Titolo**: `Latenza Predizioni (P95)`\n",
    "- **Opzioni standard -> Unit**: `seconds (s)`\n",
    "\n",
    "**Pannello 2: Richieste al secondo (Throughput)**\n",
    "- **Query PromQL**: `sum(rate(predictions_total[5m]))`\n",
    "- **Titolo**: `Throughput (req/sec)`\n",
    "\n",
    "**Pannello 3: Ripartizione Predizioni (Frode vs Non Frode)**\n",
    "- **Query PromQL**: `sum(rate(predictions_total[5m])) by (class_name)`\n",
    "- **Tipo di visualizzazione**: `Pie Chart` (Grafico a torta)\n",
    "- **Titolo**: `Ripartizione Frodi`\n",
    "\n",
    "**Salva la dashboard!** Clicca sull'icona del dischetto in alto a destra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cella 6: Simulare Traffico e Osservare la Dashboard\n",
    "\n",
    "La dashboard è pronta ma vuota. Creiamo uno script Python per simulare degli utenti che inviano dati alla nostra API. Eseguiremo questa cella direttamente dal notebook per generare traffico.\n",
    "\n",
    "Mentre la cella seguente è in esecuzione, **aggiorna la tua dashboard di Grafana**: vedrai i grafici popolarsi in tempo reale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Carichiamo i dati che useremo per la simulazione (escludendo la colonna target 'Class')\n",
    "try:\n",
    "    simulation_data = pd.read_csv('creditcard.csv').drop('Class', axis=1)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERRORE: File 'creditcard.csv' non trovato. Scaricalo da Kaggle e mettilo nella stessa cartella.\")\n",
    "    simulation_data = None\n",
    "\n",
    "if simulation_data is not None:\n",
    "    print(\"Avvio della simulazione... Invierò una richiesta ogni 0.5-2 secondi.\")\n",
    "    print(\"Premi 'Interrupt' (il quadrato nero) nel menù di Jupyter per fermare.\")\n",
    "\n",
    "    api_url = 'http://localhost:5000/predict'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Scegli una transazione a caso dal dataset\n",
    "            sample = simulation_data.sample(1)\n",
    "            \n",
    "            # Convertila in formato JSON (come richiesto dall'API)\n",
    "            payload = sample.to_dict(orient='records')[0]\n",
    "\n",
    "            try:\n",
    "                response = requests.post(api_url, json=payload, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    print(f\"Richiesta inviata. Risposta: {result['class_name']}\")\n",
    "                else:\n",
    "                    print(f\"Errore dall'API: {response.status_code}\")\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                print(\"Errore di connessione. L'app Flask è in esecuzione? Interrompo la simulazione.\")\n",
    "                break\n",
    "\n",
    "            # Aspetta un tempo casuale prima della prossima richiesta\n",
    "            time.sleep(random.uniform(0.5, 2.0))\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nSimulazione interrotta dall'utente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cella 7: Discussione e Conclusioni\n",
    "\n",
    "Abbiamo costruito una pipeline di monitoraggio di base. Riassumiamo cosa abbiamo fatto e perché è importante per il \"Testing in MLOps\".\n",
    "\n",
    "1.  **Abbiamo addestrato e servito un modello**: Questo è il primo passo di MLOps: rendere il modello utilizzabile.\n",
    "2.  **Abbiamo instrumentato l'API**: Abbiamo aggiunto metriche per avere visibilità sul suo comportamento.\n",
    "3.  **Abbiamo visualizzato le metriche**: Abbiamo dato un senso ai dati grezzi con una dashboard.\n",
    "\n",
    "**Come si collega al Testing?**\n",
    "\n",
    "Questo è **testing continuo in produzione**. Le metriche ci aiutano a rispondere a domande critiche:\n",
    "\n",
    "- **Il servizio è funzionante? (Health Check)**\n",
    "  - La metrica `up` in Prometheus (query: `up{job=\"ml-model-app\"}`) ci dice se l'app risponde. Il nostro grafico di *Throughput* è un altro ottimo indicatore di salute.\n",
    "\n",
    "- **Il servizio è performante? (Performance Testing)**\n",
    "  - La *Latenza* è la metrica chiave. Se sale, l'esperienza utente peggiora. Possiamo impostare un alert se supera una certa soglia (es. 500ms).\n",
    "\n",
    "- **Il modello si comporta in modo anomalo? (Model Behavior Testing)**\n",
    "  - La *Ripartizione delle Predizioni* è un potente indicatore di **data drift**. Se in addestramento le frodi erano lo 0.17% e in produzione iniziamo a prevederne il 10%, i dati in input sono probabilmente cambiati e il modello potrebbe non essere più valido. Questo è un segnale per investigare e, se necessario, ri-addestrare.\n",
    "\n",
    "**Passi Successivi in un Sistema Reale**\n",
    "\n",
    "1.  **Alerting**: Configurare alert automatici in Grafana o Alertmanager (es. \"Se la latenza P95 > 0.5s per 5 minuti, invia una notifica su Slack\").\n",
    "2.  **Monitoraggio della Qualità Reale (Accuracy, Precision, etc.)**: Richiede un meccanismo di feedback per ottenere il \"ground truth\". Questo è un task più complesso che implica salvare le predizioni e confrontarle con i dati reali quando diventano disponibili.\n",
    "3.  **A/B Testing**: Si possono deployare due versioni del modello e aggiungere un'etichetta `version` alle metriche per confrontare le loro performance in tempo reale sulla stessa dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
