{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Testing in MLOps: Data Testing e Model Testing\n",
        "\n",
        "In questo notebook esploreremo le principali tipologie di test in MLOps, con esempi pratici che potrete eseguire direttamente.\n",
        "\n",
        "Il testing √® un aspetto fondamentale nei progetti di Machine Learning in produzione. Mentre nel software tradizionale testiamo principalmente il codice, in MLOps dobbiamo testare anche:\n",
        "- **I dati** (qualit√†, completezza, drift)\n",
        "- **Il modello** (performance, robustezza, fairness)\n",
        "- **L'integrazione** tra componenti\n",
        "\n",
        "## Installazione delle librerie necessarie\n",
        "\n",
        "Prima di iniziare, installiamo le librerie necessarie. La cella successiva contiene i comandi pip per installare tutti i pacchetti richiesti:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decommentate le righe seguenti per installare le librerie necessarie\n",
        "# !pip install pandas numpy scikit-learn pytest great-expectations matplotlib seaborn\n",
        "\n",
        "# Librerie principali che useremo:\n",
        "# - pandas, numpy: manipolazione dati\n",
        "# - scikit-learn: modelli ML e metriche\n",
        "# - pytest: framework di testing\n",
        "# - matplotlib, seaborn: visualizzazioni\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import delle librerie necessarie per il testing in MLOps\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Settiamo il seed per garantire riproducibilit√† degli esempi\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Librerie importate con successo!\")\n",
        "print(\"üîß Setup completato - pronti per iniziare i test!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Data Testing (Test sui Dati)\n",
        "\n",
        "I test sui dati sono fondamentali per garantire la qualit√† e l'affidabilit√† dei dati che alimentano i nostri modelli.\n",
        "\n",
        "I **data test** devono verificare:\n",
        "- **Schema e formato**: struttura corretta dei dati\n",
        "- **Qualit√† e completezza**: assenza di valori nulli, duplicati, inconsistenze\n",
        "- **Plausibilit√† statistica**: valori entro range sensati\n",
        "- **Drift detection**: cambiamenti nella distribuzione dei dati\n",
        "\n",
        "### 1.1 Creazione di un dataset di esempio\n",
        "\n",
        "La prossima cella creer√† un dataset sintetico con alcune anomalie intenzionali che useremo per dimostrare i vari tipi di test sui dati.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creiamo un dataset sintetico per i nostri test\n",
        "def create_sample_dataset() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Crea un dataset di esempio con alcune anomalie intenzionali.\n",
        "    \n",
        "    Questo dataset simula dati reali e include deliberatamente:\n",
        "    - Valori nulli in alcune righe\n",
        "    - Outlier impossibili (et√† > 120, salari negativi)\n",
        "    - Righe duplicate\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Dataset con anomalie per testing\n",
        "    \"\"\"\n",
        "    \n",
        "    # Dataset base con distribuzione realistica\n",
        "    data = {\n",
        "        'et√†': np.random.randint(18, 80, 1000),\n",
        "        'salario': np.random.normal(50000, 15000, 1000),\n",
        "        'anni_esperienza': np.random.randint(0, 40, 1000),\n",
        "        'score_credito': np.random.randint(300, 850, 1000),\n",
        "        'categoria': np.random.choice(['A', 'B', 'C'], 1000)\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # ANOMALIE INTENZIONALI per testare i nostri controlli:\n",
        "    \n",
        "    # 1. Valori nulli (righe 10-15)\n",
        "    df.loc[10:15, 'salario'] = np.nan\n",
        "    \n",
        "    # 2. Outlier impossibili\n",
        "    df.loc[20, 'et√†'] = 150  # Et√† impossibile\n",
        "    df.loc[21, 'salario'] = -5000  # Salario negativo\n",
        "    \n",
        "    # 3. Righe duplicate (per testare controllo duplicati)\n",
        "    df.loc[998] = df.loc[997]\n",
        "    df.loc[999] = df.loc[997]\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Creiamo il dataset\n",
        "print(\"üîß Creazione del dataset di esempio...\")\n",
        "df_original = create_sample_dataset()\n",
        "print(f\"‚úÖ Dataset creato con {len(df_original)} righe e {len(df_original.columns)} colonne\")\n",
        "print(\"\\nPrime 5 righe:\")\n",
        "df_original.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 1.2 Test di Schema e Formato\n",
        "\n",
        "I **test di schema** verificano che i dati abbiano la struttura attesa:\n",
        "- Presenza di tutte le colonne necessarie\n",
        "- Tipi di dato corretti per ogni colonna\n",
        "- Numero minimo di righe\n",
        "\n",
        "Questo √® il primo livello di controllo - se fallisce, i dati non sono utilizzabili.\n",
        "\n",
        "La prossima cella implementa una funzione che verifica tutti questi aspetti:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_schema_formato(df: pd.DataFrame) -> Dict[str, bool]:\n",
        "    \"\"\"\n",
        "    Verifica che il dataframe rispetti lo schema atteso.\n",
        "    \n",
        "    Controlla:\n",
        "    1. Presenza di tutte le colonne necessarie\n",
        "    2. Tipi di dato corretti per ogni colonna  \n",
        "    3. Numero minimo di righe per validit√† statistica\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame da testare\n",
        "    \n",
        "    Returns:\n",
        "        Dict con risultati dei test (True = PASS, False = FAIL)\n",
        "    \"\"\"\n",
        "    \n",
        "    test_results = {}\n",
        "    \n",
        "    # Test 1: Verifica delle colonne attese\n",
        "    colonne_attese = ['et√†', 'salario', 'anni_esperienza', 'score_credito', 'categoria']\n",
        "    colonne_presenti = list(df.columns)\n",
        "    test_results['colonne_corrette'] = set(colonne_attese) == set(colonne_presenti)\n",
        "    \n",
        "    if not test_results['colonne_corrette']:\n",
        "        mancanti = set(colonne_attese) - set(colonne_presenti)\n",
        "        extra = set(colonne_presenti) - set(colonne_attese)\n",
        "        if mancanti:\n",
        "            print(f\"‚ö†Ô∏è Colonne mancanti: {mancanti}\")\n",
        "        if extra:\n",
        "            print(f\"‚ö†Ô∏è Colonne extra: {extra}\")\n",
        "    \n",
        "    # Test 2: Verifica dei tipi di dato\n",
        "    tipi_attesi = {\n",
        "        'et√†': 'int64',\n",
        "        'salario': 'float64',\n",
        "        'anni_esperienza': 'int64',\n",
        "        'score_credito': 'int64',\n",
        "        'categoria': 'object'\n",
        "    }\n",
        "    \n",
        "    tipi_corretti = True\n",
        "    for col, tipo_atteso in tipi_attesi.items():\n",
        "        if col in df.columns:\n",
        "            if df[col].dtype != tipo_atteso:\n",
        "                tipi_corretti = False\n",
        "                print(f\"‚ö†Ô∏è Colonna '{col}': tipo {df[col].dtype} invece di {tipo_atteso}\")\n",
        "    \n",
        "    test_results['tipi_corretti'] = tipi_corretti\n",
        "    \n",
        "    # Test 3: Verifica del numero minimo di righe\n",
        "    test_results['righe_sufficienti'] = len(df) >= 100\n",
        "    \n",
        "    return test_results\n",
        "\n",
        "# Eseguiamo i test di schema\n",
        "print(\"üîç Esecuzione test di schema e formato...\")\n",
        "risultati_schema = test_schema_formato(df_original)\n",
        "print(\"\\nRisultati Test di Schema:\")\n",
        "for test, risultato in risultati_schema.items():\n",
        "    stato = \"‚úÖ PASS\" if risultato else \"‚ùå FAIL\"\n",
        "    print(f\"  {test}: {stato}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 1.3 Test di Qualit√† e Completezza\n",
        "\n",
        "I **test di qualit√†** verificano l'integrit√† dei dati:\n",
        "- **Valori nulli**: presenza di missing values\n",
        "- **Duplicati**: righe identiche che potrebbero causare overfitting\n",
        "- **Valori categorici**: range di valori validi per variabili categoriche\n",
        "\n",
        "Questi test ci dicono se i dati sono \"puliti\" e pronti per l'uso.\n",
        "\n",
        "La prossima cella implementa controlli dettagliati per tutti questi aspetti:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_qualita_completezza(df: pd.DataFrame) -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    Verifica la qualit√† e completezza dei dati.\n",
        "    \n",
        "    Controlla:\n",
        "    1. Valori nulli in tutte le colonne\n",
        "    2. Presenza di righe duplicate\n",
        "    3. Validit√† dei valori categorici\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame da testare\n",
        "    \n",
        "    Returns:\n",
        "        Dict con risultati dettagliati dei test\n",
        "    \"\"\"\n",
        "    \n",
        "    risultati = {}\n",
        "    \n",
        "    # Test 1: Controllo valori nulli\n",
        "    valori_nulli = df.isnull().sum()\n",
        "    risultati['valori_nulli'] = {\n",
        "        'totale': valori_nulli.sum(),\n",
        "        'per_colonna': valori_nulli.to_dict(),\n",
        "        'test_passed': valori_nulli.sum() == 0\n",
        "    }\n",
        "    \n",
        "    # Test 2: Controllo duplicati\n",
        "    duplicati = df.duplicated().sum()\n",
        "    risultati['duplicati'] = {\n",
        "        'totale': duplicati,\n",
        "        'percentuale': (duplicati / len(df)) * 100,\n",
        "        'test_passed': duplicati == 0\n",
        "    }\n",
        "    \n",
        "    # Test 3: Controllo valori unici per colonne categoriche\n",
        "    valori_unici_categoria = df['categoria'].nunique()\n",
        "    risultati['valori_unici_categoria'] = {\n",
        "        'numero': valori_unici_categoria,\n",
        "        'valori': df['categoria'].unique().tolist(),\n",
        "        'test_passed': valori_unici_categoria >= 2 and valori_unici_categoria <= 10\n",
        "    }\n",
        "    \n",
        "    return risultati\n",
        "\n",
        "# Eseguiamo i test di qualit√†\n",
        "print(\"üîç Esecuzione test di qualit√† e completezza...\")\n",
        "risultati_qualita = test_qualita_completezza(df_original)\n",
        "\n",
        "print(\"\\nüìä Risultati Test di Qualit√† e Completezza:\\n\")\n",
        "print(f\"1. Valori Nulli:\")\n",
        "print(f\"   - Totale: {risultati_qualita['valori_nulli']['totale']}\")\n",
        "print(f\"   - Per colonna: {risultati_qualita['valori_nulli']['per_colonna']}\")\n",
        "print(f\"   - Test: {'‚úÖ PASS' if risultati_qualita['valori_nulli']['test_passed'] else '‚ùå FAIL'}\")\n",
        "\n",
        "print(f\"\\n2. Duplicati:\")\n",
        "print(f\"   - Totale: {risultati_qualita['duplicati']['totale']}\")\n",
        "print(f\"   - Percentuale: {risultati_qualita['duplicati']['percentuale']:.2f}%\")\n",
        "print(f\"   - Test: {'‚úÖ PASS' if risultati_qualita['duplicati']['test_passed'] else '‚ùå FAIL'}\")\n",
        "\n",
        "print(f\"\\n3. Valori Categorici:\")\n",
        "print(f\"   - Valori unici: {risultati_qualita['valori_unici_categoria']['numero']}\")\n",
        "print(f\"   - Valori: {risultati_qualita['valori_unici_categoria']['valori']}\")\n",
        "print(f\"   - Test: {'‚úÖ PASS' if risultati_qualita['valori_unici_categoria']['test_passed'] else '‚ùå FAIL'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 1.4 Test di Plausibilit√† Statistica\n",
        "\n",
        "I **test di plausibilit√†** verificano che i valori siano statisticamente sensati:\n",
        "- **Range validation**: valori entro limiti logici (es. et√† 0-120)\n",
        "- **Distribuzione ragionevole**: assenza di outlier estremi\n",
        "- **Coerenza business**: valori che hanno senso nel contesto\n",
        "\n",
        "Questi test catturano errori di data entry o problemi di integrazione.\n",
        "\n",
        "La prossima cella implementa controlli di range per tutte le variabili numeriche:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_plausibilita_statistica(df: pd.DataFrame) -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    Verifica che i valori rientrino in range plausibili.\n",
        "    \n",
        "    Controlla ogni colonna numerica per:\n",
        "    1. Valori entro range business-logic\n",
        "    2. Identificazione di outlier estremi\n",
        "    3. Coerenza con aspettative di dominio\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame da testare\n",
        "    \n",
        "    Returns:\n",
        "        Dict con risultati dettagliati per ogni colonna\n",
        "    \"\"\"\n",
        "    \n",
        "    risultati = {}\n",
        "    \n",
        "    # Definizione dei range plausibili (da knowledge del dominio)\n",
        "    range_plausibili = {\n",
        "        'et√†': (0, 120),           # Range umano normale\n",
        "        'salario': (0, 500000),    # Range salari plausibili\n",
        "        'anni_esperienza': (0, 60), # Max esperienza lavorativa\n",
        "        'score_credito': (300, 850) # Range standard score credito\n",
        "    }\n",
        "    \n",
        "    for colonna, (min_val, max_val) in range_plausibili.items():\n",
        "        if colonna in df.columns:\n",
        "            # Rimuoviamo i NaN per il calcolo\n",
        "            valori = df[colonna].dropna()\n",
        "            \n",
        "            # Troviamo valori fuori range\n",
        "            fuori_range = valori[(valori < min_val) | (valori > max_val)]\n",
        "            \n",
        "            risultati[colonna] = {\n",
        "                'min_valore': valori.min(),\n",
        "                'max_valore': valori.max(),\n",
        "                'range_atteso': (min_val, max_val),\n",
        "                'valori_fuori_range': len(fuori_range),\n",
        "                'test_passed': len(fuori_range) == 0\n",
        "            }\n",
        "            \n",
        "            # Se ci sono valori fuori range, mostriamoli\n",
        "            if len(fuori_range) > 0:\n",
        "                risultati[colonna]['esempi_fuori_range'] = fuori_range.head().tolist()\n",
        "    \n",
        "    return risultati\n",
        "\n",
        "# Eseguiamo i test di plausibilit√†\n",
        "print(\"üîç Esecuzione test di plausibilit√† statistica...\")\n",
        "risultati_plausibilita = test_plausibilita_statistica(df_original)\n",
        "\n",
        "print(\"\\nüìä Risultati Test di Plausibilit√† Statistica:\\n\")\n",
        "for colonna, risultato in risultati_plausibilita.items():\n",
        "    print(f\"{colonna}:\")\n",
        "    print(f\"  - Range atteso: {risultato['range_atteso']}\")\n",
        "    print(f\"  - Range effettivo: ({risultato['min_valore']:.0f}, {risultato['max_valore']:.0f})\")\n",
        "    print(f\"  - Valori fuori range: {risultato['valori_fuori_range']}\")\n",
        "    if risultato['valori_fuori_range'] > 0:\n",
        "        print(f\"  - Esempi: {risultato.get('esempi_fuori_range', [])}\")\n",
        "    print(f\"  - Test: {'‚úÖ PASS' if risultato['test_passed'] else '‚ùå FAIL'}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 1.5 Test di Drift (Simulazione)\n",
        "\n",
        "Il **data drift** √® uno dei problemi pi√π comuni in produzione: i dati cambiano nel tempo e il modello pu√≤ degradare.\n",
        "\n",
        "**Tipi di drift**:\n",
        "- **Covariate drift**: cambiano le distribuzioni delle feature (X)\n",
        "- **Concept drift**: cambia la relazione X -> Y\n",
        "- **Label drift**: cambia la distribuzione del target (Y)\n",
        "\n",
        "La prossima cella simula un dataset con drift e implementa test statistici per rilevarlo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simula_data_drift() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Simula un dataset con drift rispetto all'originale.\n",
        "    \n",
        "    Crea due dataset:\n",
        "    1. Training: distribuzione originale\n",
        "    2. Produzione: distribuzione con drift simulato\n",
        "    \n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: (df_train, df_prod)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Dataset originale (training) - distribuzione baseline\n",
        "    df_train = pd.DataFrame({\n",
        "        'et√†': np.random.normal(40, 10, 1000),\n",
        "        'salario': np.random.normal(50000, 15000, 1000),\n",
        "        'score_credito': np.random.normal(650, 100, 1000)\n",
        "    })\n",
        "    \n",
        "    # Dataset con drift (produzione) - distribuzione cambiata\n",
        "    # Simuliamo cambiamenti tipici che potrebbero accadere in produzione\n",
        "    df_prod = pd.DataFrame({\n",
        "        'et√†': np.random.normal(35, 12, 1000),      # Popolazione pi√π giovane\n",
        "        'salario': np.random.normal(55000, 20000, 1000),  # Salari pi√π alti e variabili\n",
        "        'score_credito': np.random.normal(620, 120, 1000)  # Score pi√π bassi e variabili\n",
        "    })\n",
        "    \n",
        "    return df_train, df_prod\n",
        "\n",
        "def test_data_drift(df_train: pd.DataFrame, df_prod: pd.DataFrame, \n",
        "                   threshold: float = 0.1) -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    Testa il drift tra due dataset usando test statistici semplici.\n",
        "    \n",
        "    Confronta:\n",
        "    1. Media delle distribuzioni\n",
        "    2. Deviazione standard delle distribuzioni\n",
        "    3. Valuta se il drift supera soglie predefinite\n",
        "    \n",
        "    Args:\n",
        "        df_train: Dataset di training (baseline)\n",
        "        df_prod: Dataset di produzione (confronto)\n",
        "        threshold: Soglia per rilevare drift significativo\n",
        "    \n",
        "    Returns:\n",
        "        Dict con risultati del test per ogni colonna\n",
        "    \"\"\"\n",
        "    \n",
        "    risultati = {}\n",
        "    \n",
        "    for colonna in df_train.columns:\n",
        "        if colonna in df_prod.columns:\n",
        "            # Calcolo statistiche di base\n",
        "            mean_train = df_train[colonna].mean()\n",
        "            mean_prod = df_prod[colonna].mean()\n",
        "            std_train = df_train[colonna].std()\n",
        "            std_prod = df_prod[colonna].std()\n",
        "            \n",
        "            # Drift della media (normalizzato per essere scale-invariant)\n",
        "            drift_mean = abs(mean_prod - mean_train) / mean_train if mean_train != 0 else 0\n",
        "            \n",
        "            # Drift della deviazione standard\n",
        "            drift_std = abs(std_prod - std_train) / std_train if std_train != 0 else 0\n",
        "            \n",
        "            risultati[colonna] = {\n",
        "                'mean_train': mean_train,\n",
        "                'mean_prod': mean_prod,\n",
        "                'drift_mean_pct': drift_mean * 100,\n",
        "                'drift_std_pct': drift_std * 100,\n",
        "                'drift_detected': drift_mean > threshold or drift_std > threshold\n",
        "            }\n",
        "    \n",
        "    return risultati\n",
        "\n",
        "# Simuliamo e testiamo il drift\n",
        "print(\"üîç Simulazione e test del data drift...\")\n",
        "df_train, df_prod = simula_data_drift()\n",
        "risultati_drift = test_data_drift(df_train, df_prod)\n",
        "\n",
        "print(\"\\nüîÑ Risultati Test di Data Drift:\\n\")\n",
        "for colonna, risultato in risultati_drift.items():\n",
        "    print(f\"{colonna}:\")\n",
        "    print(f\"  - Media training: {risultato['mean_train']:.2f}\")\n",
        "    print(f\"  - Media produzione: {risultato['mean_prod']:.2f}\")\n",
        "    print(f\"  - Drift media: {risultato['drift_mean_pct']:.1f}%\")\n",
        "    print(f\"  - Drift std: {risultato['drift_std_pct']:.1f}%\")\n",
        "    print(f\"  - Drift rilevato: {'‚ö†Ô∏è SI' if risultato['drift_detected'] else '‚úÖ NO'}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizziamo il drift con istogrammi comparativi\n",
        "print(\"üìä Creazione visualizzazione comparativa del drift...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "for idx, col in enumerate(['et√†', 'salario', 'score_credito']):\n",
        "    # Istogrammi sovrapposti per confronto visivo\n",
        "    axes[idx].hist(df_train[col], bins=30, alpha=0.6, label='Training', density=True, color='blue')\n",
        "    axes[idx].hist(df_prod[col], bins=30, alpha=0.6, label='Produzione', density=True, color='red')\n",
        "    \n",
        "    # Linee verticali per le medie\n",
        "    axes[idx].axvline(df_train[col].mean(), color='blue', linestyle='--', alpha=0.8, label='Media Train')\n",
        "    axes[idx].axvline(df_prod[col].mean(), color='red', linestyle='--', alpha=0.8, label='Media Prod')\n",
        "    \n",
        "    axes[idx].set_title(f'Distribuzione {col}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel(col)\n",
        "    axes[idx].set_ylabel('Densit√†')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Confronto Distribuzioni: Training vs Produzione', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualizzazione completata!\")\n",
        "print(\"\\nüí° Osservazioni:\")\n",
        "print(\"- Le aree blu mostrano la distribuzione originale (training)\")\n",
        "print(\"- Le aree rosse mostrano la distribuzione con drift (produzione)\")\n",
        "print(\"- Le linee tratteggiate indicano le medie delle distribuzioni\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Model Testing (Test sul Modello)\n",
        "\n",
        "Ora passiamo ai test sul modello stesso, che valutano performance, robustezza e fairness.\n",
        "\n",
        "I **model test** devono verificare:\n",
        "- **Performance**: metriche sopra soglie minime accettabili\n",
        "- **Robustezza**: stabilit√† con dati perturbati o incompleti\n",
        "- **Stabilit√†**: risultati consistenti con diversi seed/configurazioni\n",
        "- **Fairness**: equit√† tra diversi gruppi demografici\n",
        "- **Slicing**: performance su sottopopolazioni specifiche\n",
        "\n",
        "### 2.1 Preparazione del modello di esempio\n",
        "\n",
        "La prossima cella crea un dataset sintetico per la classificazione e addestra un modello RandomForest che useremo per tutti i test successivi:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creiamo un dataset sintetico per il nostro modello di classificazione\n",
        "print(\"üîß Creazione dataset sintetico per model testing...\")\n",
        "\n",
        "# Dataset con 1000 campioni, 10 features, 8 informative\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, \n",
        "    n_features=10, \n",
        "    n_informative=8,      # Features correlate al target\n",
        "    n_redundant=2,        # Features ridondanti\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Creiamo un DataFrame completo con metadati per test avanzati\n",
        "df_model = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])\n",
        "df_model['target'] = y\n",
        "\n",
        "# Aggiungiamo gruppi demografici per test di fairness\n",
        "# Gruppo A: 60%, Gruppo B: 40% (riflette sbilanciamento reale)\n",
        "df_model['gruppo'] = np.random.choice(['A', 'B'], size=1000, p=[0.6, 0.4])\n",
        "\n",
        "print(f\"‚úÖ Dataset creato: {df_model.shape[0]} campioni, {df_model.shape[1]-2} features\")\n",
        "print(f\"   - Target distribution: {dict(df_model['target'].value_counts())}\")\n",
        "print(f\"   - Gruppi demografici: {dict(df_model['gruppo'].value_counts())}\")\n",
        "\n",
        "# Split dei dati mantenendo gli indici originali\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_model.drop(['target', 'gruppo'], axis=1), \n",
        "    df_model['target'], \n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=df_model['target']  # Manteniamo la distribuzione del target\n",
        ")\n",
        "\n",
        "print(f\"\\\\nüìä Split completato:\")\n",
        "print(f\"   - Training set: {X_train.shape[0]} campioni\")\n",
        "print(f\"   - Test set: {X_test.shape[0]} campioni\")\n",
        "\n",
        "# Training del modello\n",
        "print(\"\\\\nü§ñ Addestramento RandomForest...\")\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Valutazione iniziale\n",
        "accuracy_baseline = accuracy_score(y_test, model.predict(X_test))\n",
        "print(f\"‚úÖ Modello addestrato con successo!\")\n",
        "print(f\"   - Accuracy baseline sul test set: {accuracy_baseline:.3f}\")\n",
        "print(f\"   - Pronto per i test di robustezza e fairness\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2.2 Test di Performance\n",
        "\n",
        "I **test di performance** verificano che il modello raggiunga standard minimi accettabili:\n",
        "- **Accuracy**: percentuale di predizioni corrette\n",
        "- **Precision**: correttezza delle predizioni positive\n",
        "- **Recall**: capacit√† di trovare tutti i casi positivi\n",
        "- **F1-Score**: bilancio tra precision e recall\n",
        "\n",
        "Questi test implementano **acceptance criteria** - soglie sotto cui il modello non √® accettabile per la produzione.\n",
        "\n",
        "La prossima cella implementa controlli automatici per tutte le metriche principali:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_performance_modello(model, X_test, y_test, \n",
        "                           soglie_minime: Dict[str, float] = None) -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    Testa le performance del modello rispetto a soglie minime (acceptance criteria).\n",
        "    \n",
        "    Valuta se il modello √® accettabile per la produzione basandosi su:\n",
        "    1. Accuracy minima richiesta\n",
        "    2. Precision minima (riduce falsi positivi)\n",
        "    3. Recall minima (riduce falsi negativi)\n",
        "    4. F1-Score minimo (bilancio generale)\n",
        "    \n",
        "    Args:\n",
        "        model: Modello addestrato\n",
        "        X_test: Features di test\n",
        "        y_test: Target di test\n",
        "        soglie_minime: Dict con soglie per ogni metrica\n",
        "    \n",
        "    Returns:\n",
        "        Dict con risultati dettagliati per ogni metrica\n",
        "    \"\"\"\n",
        "    \n",
        "    # Soglie di default (da definire con business stakeholders)\n",
        "    if soglie_minime is None:\n",
        "        soglie_minime = {\n",
        "            'accuracy': 0.7,    # 70% accuracy minima\n",
        "            'precision': 0.7,   # 70% precision minima\n",
        "            'recall': 0.7,      # 70% recall minima\n",
        "            'f1': 0.7          # 70% F1-score minima\n",
        "        }\n",
        "    \n",
        "    # Predizioni\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calcolo metriche standard\n",
        "    metriche = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
        "    }\n",
        "    \n",
        "    # Test rispetto alle soglie (acceptance criteria)\n",
        "    risultati = {}\n",
        "    for metrica, valore in metriche.items():\n",
        "        soglia = soglie_minime.get(metrica, 0.7)\n",
        "        risultati[metrica] = {\n",
        "            'valore': valore,\n",
        "            'soglia_minima': soglia,\n",
        "            'test_passed': valore >= soglia,\n",
        "            'margine': valore - soglia  # Quanto sopra/sotto la soglia\n",
        "        }\n",
        "    \n",
        "    return risultati\n",
        "\n",
        "# Eseguiamo i test di performance\n",
        "print(\"üîç Esecuzione test di performance...\")\n",
        "risultati_performance = test_performance_modello(model, X_test, y_test)\n",
        "\n",
        "print(\"\\nüìà Risultati Test di Performance del Modello:\\n\")\n",
        "for metrica, risultato in risultati_performance.items():\n",
        "    stato = \"‚úÖ PASS\" if risultato['test_passed'] else \"‚ùå FAIL\"\n",
        "    margine_str = f\"(+{risultato['margine']:.3f})\" if risultato['margine'] > 0 else f\"({risultato['margine']:.3f})\"\n",
        "    \n",
        "    print(f\"{metrica.capitalize()}:\")\n",
        "    print(f\"  - Valore: {risultato['valore']:.3f}\")\n",
        "    print(f\"  - Soglia minima: {risultato['soglia_minima']:.3f}\")\n",
        "    print(f\"  - Margine: {margine_str}\")\n",
        "    print(f\"  - Test: {stato}\")\n",
        "    print()\n",
        "\n",
        "# Verifica generale\n",
        "tutti_pass = all(risultato['test_passed'] for risultato in risultati_performance.values())\n",
        "print(f\"üéØ Verdetto finale: {'‚úÖ MODELLO ACCETTABILE' if tutti_pass else '‚ùå MODELLO NON ACCETTABILE'}\")\n",
        "if not tutti_pass:\n",
        "    print(\"   ‚ö†Ô∏è Il modello non raggiunge gli standard minimi richiesti per la produzione\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2.3 Test di Robustezza (Stress Test)\n",
        "\n",
        "I **test di robustezza** verificano come il modello si comporta con dati imperfetti o perturbati:\n",
        "- **Rumore gaussiano**: aggiunta di rumore casuale alle features\n",
        "- **Valori mancanti**: simulazione di missing values\n",
        "- **Outlier**: valori estremi o anomali\n",
        "- **Corrupted data**: dati parzialmente danneggiati\n",
        "\n",
        "Questi test simulano condizioni reali di produzione dove i dati potrebbero essere meno \"puliti\" del training set.\n",
        "\n",
        "La prossima cella implementa stress test sistematici con diversi livelli di perturbazione:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_robustezza_modello(model, X_test, y_test) -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    Testa la robustezza del modello con dati perturbati (stress test).\n",
        "    \n",
        "    Simula condizioni avverse di produzione:\n",
        "    1. Rumore gaussiano con intensit√† crescente\n",
        "    2. Valori mancanti con percentuali crescenti\n",
        "    3. Valuta la degradazione rispetto al baseline\n",
        "    \n",
        "    Args:\n",
        "        model: Modello addestrato\n",
        "        X_test: Features di test\n",
        "        y_test: Target di test\n",
        "    \n",
        "    Returns:\n",
        "        Dict con risultati dettagliati per ogni tipo di perturbazione\n",
        "    \"\"\"\n",
        "    \n",
        "    risultati = {}\n",
        "    \n",
        "    # Performance baseline (condizioni ideali)\n",
        "    y_pred_baseline = model.predict(X_test)\n",
        "    acc_baseline = accuracy_score(y_test, y_pred_baseline)\n",
        "    \n",
        "    # Test 1: Robustezza al rumore gaussiano\n",
        "    noise_levels = [0.1, 0.5, 1.0]  # Intensit√† crescente di rumore\n",
        "    risultati['rumore_gaussiano'] = {}\n",
        "    \n",
        "    for noise_level in noise_levels:\n",
        "        # Aggiungiamo rumore gaussiano alle features\n",
        "        X_noisy = X_test + np.random.normal(0, noise_level, X_test.shape)\n",
        "        y_pred_noisy = model.predict(X_noisy)\n",
        "        acc_noisy = accuracy_score(y_test, y_pred_noisy)\n",
        "        \n",
        "        # Calcolo degradazione percentuale\n",
        "        degradazione = (acc_baseline - acc_noisy) / acc_baseline * 100\n",
        "        \n",
        "        risultati['rumore_gaussiano'][f'noise_{noise_level}'] = {\n",
        "            'accuracy': acc_noisy,\n",
        "            'degradazione_pct': degradazione,\n",
        "            'test_passed': degradazione < 20  # Soglia: max 20% degradazione\n",
        "        }\n",
        "    \n",
        "    # Test 2: Robustezza ai valori mancanti\n",
        "    missing_rates = [0.1, 0.2, 0.3]  # 10%, 20%, 30% valori mancanti\n",
        "    risultati['valori_mancanti'] = {}\n",
        "    \n",
        "    for missing_rate in missing_rates:\n",
        "        X_missing = X_test.copy()\n",
        "        # Creiamo una maschera casuale per i valori mancanti\n",
        "        mask = np.random.random(X_missing.shape) < missing_rate\n",
        "        X_missing[mask] = 0  # Sostituiamo con 0 (strategia semplice)\n",
        "        \n",
        "        y_pred_missing = model.predict(X_missing)\n",
        "        acc_missing = accuracy_score(y_test, y_pred_missing)\n",
        "        \n",
        "        degradazione = (acc_baseline - acc_missing) / acc_baseline * 100\n",
        "        \n",
        "        risultati['valori_mancanti'][f'missing_{int(missing_rate*100)}pct'] = {\n",
        "            'accuracy': acc_missing,\n",
        "            'degradazione_pct': degradazione,\n",
        "            'test_passed': degradazione < 30  # Soglia: max 30% degradazione\n",
        "        }\n",
        "    \n",
        "    risultati['baseline'] = {'accuracy': acc_baseline}\n",
        "    \n",
        "    return risultati\n",
        "\n",
        "# Eseguiamo i test di robustezza\n",
        "print(\"üîç Esecuzione stress test di robustezza...\")\n",
        "risultati_robustezza = test_robustezza_modello(model, X_test, y_test)\n",
        "\n",
        "print(\"\\nüõ°Ô∏è Risultati Test di Robustezza (Stress Test):\\n\")\n",
        "print(f\"Accuracy baseline: {risultati_robustezza['baseline']['accuracy']:.3f}\")\n",
        "print(\"(Performance in condizioni ideali)\\n\")\n",
        "\n",
        "print(\"1. Test con Rumore Gaussiano:\")\n",
        "for test, risultato in risultati_robustezza['rumore_gaussiano'].items():\n",
        "    stato = \"‚úÖ PASS\" if risultato['test_passed'] else \"‚ùå FAIL\"\n",
        "    print(f\"  {test}:\")\n",
        "    print(f\"    - Accuracy: {risultato['accuracy']:.3f}\")\n",
        "    print(f\"    - Degradazione: {risultato['degradazione_pct']:.1f}%\")\n",
        "    print(f\"    - Test: {stato}\")\n",
        "\n",
        "print(\"\\n2. Test con Valori Mancanti:\")\n",
        "for test, risultato in risultati_robustezza['valori_mancanti'].items():\n",
        "    stato = \"‚úÖ PASS\" if risultato['test_passed'] else \"‚ùå FAIL\"\n",
        "    print(f\"  {test}:\")\n",
        "    print(f\"    - Accuracy: {risultato['accuracy']:.3f}\")\n",
        "    print(f\"    - Degradazione: {risultato['degradazione_pct']:.1f}%\")\n",
        "    print(f\"    - Test: {stato}\")\n",
        "\n",
        "# Verifica generale della robustezza\n",
        "tutti_rumore_pass = all(r['test_passed'] for r in risultati_robustezza['rumore_gaussiano'].values())\n",
        "tutti_missing_pass = all(r['test_passed'] for r in risultati_robustezza['valori_mancanti'].values())\n",
        "robustezza_generale = tutti_rumore_pass and tutti_missing_pass\n",
        "\n",
        "print(f\"\\nüéØ Verdetto robustezza: {'‚úÖ MODELLO ROBUSTO' if robustezza_generale else '‚ùå MODELLO FRAGILE'}\")\n",
        "if not robustezza_generale:\n",
        "    print(\"   ‚ö†Ô∏è Il modello √® sensibile a perturbazioni dei dati\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2.4 Test di Stabilit√†\n",
        "\n",
        "I **test di stabilit√†** verificano la consistenza del modello:\n",
        "- **Variabilit√† tra run**: performance simili con seed diversi\n",
        "- **Determinismo**: risultati riproducibili\n",
        "- **Agreement tra modelli**: consenso tra istanze multiple\n",
        "\n",
        "Un modello instabile pu√≤ avere performance variabili in produzione e risultati non affidabili.\n",
        "\n",
        "La prossima cella addestra pi√π istanze del modello con seed diversi per valutare la stabilit√†:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_stabilita_modello(X_train, y_train, X_test, y_test, n_runs=5):\n",
        "    \"\"\"\n",
        "    Testa la stabilit√† del modello con diversi seed casuali.\n",
        "    \n",
        "    Valuta:\n",
        "    1. Variabilit√† delle performance tra run diversi\n",
        "    2. Accordo tra predizioni di modelli diversi\n",
        "    3. Coefficiente di variazione delle metriche\n",
        "    \n",
        "    Args:\n",
        "        X_train, y_train: Dati di training\n",
        "        X_test, y_test: Dati di test\n",
        "        n_runs: Numero di run con seed diversi\n",
        "    \n",
        "    Returns:\n",
        "        Tuple[Dict, List]: (risultati statistici, lista accuracy)\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"üîÑ Addestramento di {n_runs} modelli con seed diversi...\")\n",
        "    \n",
        "    accuracies = []\n",
        "    predictions_list = []\n",
        "    \n",
        "    for seed in range(n_runs):\n",
        "        # Addestra modello con seed diverso\n",
        "        model_temp = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
        "        model_temp.fit(X_train, y_train)\n",
        "        \n",
        "        # Valuta performance\n",
        "        y_pred = model_temp.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        \n",
        "        accuracies.append(accuracy)\n",
        "        predictions_list.append(y_pred)\n",
        "        \n",
        "        print(f\"  Run {seed+1}: Accuracy = {accuracy:.3f}\")\n",
        "    \n",
        "    # Calcola statistiche di stabilit√†\n",
        "    acc_mean = np.mean(accuracies)\n",
        "    acc_std = np.std(accuracies)\n",
        "    acc_cv = acc_std / acc_mean  # Coefficiente di variazione\n",
        "    \n",
        "    # Calcola agreement tra predizioni\n",
        "    predictions_array = np.array(predictions_list)\n",
        "    agreement_rates = []\n",
        "    \n",
        "    for i in range(len(X_test)):\n",
        "        preds_for_sample = predictions_array[:, i]\n",
        "        # Percentuale di modelli che concordano sulla predizione pi√π comune\n",
        "        most_common_pred = np.bincount(preds_for_sample).argmax()\n",
        "        agreement_rate = (preds_for_sample == most_common_pred).mean()\n",
        "        agreement_rates.append(agreement_rate)\n",
        "    \n",
        "    mean_agreement = np.mean(agreement_rates)\n",
        "    \n",
        "    risultati = {\n",
        "        'accuracy_mean': acc_mean,\n",
        "        'accuracy_std': acc_std,\n",
        "        'coefficient_variation': acc_cv,\n",
        "        'mean_agreement': mean_agreement,\n",
        "        'test_passed': acc_cv < 0.05 and mean_agreement > 0.9\n",
        "    }\n",
        "    \n",
        "    return risultati, accuracies\n",
        "\n",
        "# Eseguiamo il test di stabilit√†\n",
        "risultati_stabilita, accuracies = test_stabilita_modello(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nüìä Risultati Test di Stabilit√† del Modello:\\n\")\n",
        "print(f\"Accuracy media: {risultati_stabilita['accuracy_mean']:.3f} ¬± {risultati_stabilita['accuracy_std']:.3f}\")\n",
        "print(f\"Coefficiente di variazione: {risultati_stabilita['coefficient_variation']:.3f}\")\n",
        "print(f\"Agreement medio tra modelli: {risultati_stabilita['mean_agreement']:.1%}\")\n",
        "print(f\"\\nTest: {'‚úÖ PASS' if risultati_stabilita['test_passed'] else '‚ùå FAIL'}\")\n",
        "print(f\"(Criteri: CV < 0.05 e Agreement > 90%)\")\n",
        "\n",
        "# Visualizzazione della stabilit√†\n",
        "print(\"\\nüìà Creazione visualizzazioni di stabilit√†...\")\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Grafico 1: Andamento accuracy tra run\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(accuracies)+1), accuracies, 'o-', color='blue', linewidth=2, markersize=8)\n",
        "plt.axhline(y=risultati_stabilita['accuracy_mean'], color='red', linestyle='--', alpha=0.8, label=f'Media: {risultati_stabilita[\"accuracy_mean\"]:.3f}')\n",
        "plt.fill_between(range(1, len(accuracies)+1), \n",
        "                 risultati_stabilita['accuracy_mean'] - risultati_stabilita['accuracy_std'],\n",
        "                 risultati_stabilita['accuracy_mean'] + risultati_stabilita['accuracy_std'],\n",
        "                 alpha=0.2, color='red', label='¬±1 œÉ')\n",
        "plt.xlabel('Run')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Stabilit√† Accuracy tra Run', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Grafico 2: Distribuzione accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(accuracies, bins=max(3, len(accuracies)//2), edgecolor='black', alpha=0.7, color='skyblue')\n",
        "plt.axvline(risultati_stabilita['accuracy_mean'], color='red', linestyle='--', alpha=0.8, label=f'Media: {risultati_stabilita[\"accuracy_mean\"]:.3f}')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Frequenza')\n",
        "plt.title('Distribuzione Accuracy', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualizzazioni completate!\")\n",
        "print(f\"\\nüí° Interpretazione:\")\n",
        "print(f\"- CV basso ({risultati_stabilita['coefficient_variation']:.3f}) indica performance consistenti\")\n",
        "print(f\"- Agreement alto ({risultati_stabilita['mean_agreement']:.1%}) indica predizioni affidabili\")\n",
        "print(f\"- Modello {'stabile' if risultati_stabilita['test_passed'] else 'instabile'} per la produzione\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2.5 Test di Fairness\n",
        "\n",
        "I **test di fairness** verificano l'equit√† del modello tra diversi gruppi:\n",
        "- **Demographic parity**: tassi di predizione simili tra gruppi\n",
        "- **Equalized odds**: performance simili tra gruppi\n",
        "- **Calibration**: predizioni ugualmente affidabili\n",
        "\n",
        "La fairness √® cruciale per evitare discriminazioni e bias sistemici.\n",
        "\n",
        "La prossima cella implementa test di fairness confrontando le performance tra gruppi demografici:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### üí° Nota Tecnica: Gestione degli Indici in Pandas\n",
        "\n",
        "Quando lavoriamo con train/test split e vogliamo mantenere la traccia di gruppi demografici, dobbiamo fare attenzione all'indicizzazione:\n",
        "\n",
        "- **`.iloc`**: Indicizzazione posizionale (0, 1, 2, ...)\n",
        "- **`.loc`**: Indicizzazione basata su etichette (indici originali del DataFrame)\n",
        "\n",
        "Il train/test split mantiene gli indici originali, quindi dobbiamo usare `.loc` per accedere correttamente ai dati.\n",
        "\n",
        "**Esempio**:\n",
        "```python\n",
        "# Corretto: usa .loc per indici originali\n",
        "df_test_groups = df_model.loc[X_test.index]['gruppo']\n",
        "\n",
        "# Errato: .iloc causerebbe IndexError\n",
        "# df_test_groups = df_model.iloc[X_test.index]['gruppo']\n",
        "```\n",
        "\n",
        "La prossima cella implementa i test di fairness con la gestione corretta degli indici:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_fairness_modello(model, X_test, y_test, df_test_groups):\n",
        "    \"\"\"\n",
        "    Testa la fairness del modello tra diversi gruppi demografici.\n",
        "    \n",
        "    Valuta:\n",
        "    1. Performance per ogni gruppo demografico\n",
        "    2. Disparit√† tra gruppi (accuracy, positive rate)\n",
        "    3. Soglie di accettabilit√† per fairness\n",
        "    \n",
        "    Args:\n",
        "        model: Modello addestrato\n",
        "        X_test: Features di test\n",
        "        y_test: Target di test\n",
        "        df_test_groups: DataFrame con info demografiche per test set\n",
        "    \n",
        "    Returns:\n",
        "        Tuple[Dict, Dict]: (risultati per gruppo, test disparit√†)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Predizioni\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calcola metriche per gruppo\n",
        "    risultati_per_gruppo = {}\n",
        "    \n",
        "    for gruppo in df_test_groups['gruppo'].unique():\n",
        "        mask = df_test_groups['gruppo'] == gruppo\n",
        "        indices = df_test_groups[mask].index\n",
        "        \n",
        "        # Troviamo le posizioni corrette nel test set usando .loc\n",
        "        # Manteniamo la corrispondenza tra indici originali e posizioni nel test set\n",
        "        test_positions = [list(y_test.index).index(idx) for idx in indices if idx in y_test.index]\n",
        "        \n",
        "        y_true_gruppo = y_test.iloc[test_positions]\n",
        "        y_pred_gruppo = y_pred[test_positions]\n",
        "        \n",
        "        risultati_per_gruppo[gruppo] = {\n",
        "            'n_samples': len(y_true_gruppo),\n",
        "            'accuracy': accuracy_score(y_true_gruppo, y_pred_gruppo),\n",
        "            'precision': precision_score(y_true_gruppo, y_pred_gruppo, average='weighted'),\n",
        "            'recall': recall_score(y_true_gruppo, y_pred_gruppo, average='weighted'),\n",
        "            'positive_rate': (y_pred_gruppo == 1).mean()\n",
        "        }\n",
        "    \n",
        "    # Calcola disparit√† tra gruppi\n",
        "    gruppi = list(risultati_per_gruppo.keys())\n",
        "    if len(gruppi) >= 2:\n",
        "        acc_diff = abs(risultati_per_gruppo[gruppi[0]]['accuracy'] - \n",
        "                      risultati_per_gruppo[gruppi[1]]['accuracy'])\n",
        "        pr_diff = abs(risultati_per_gruppo[gruppi[0]]['positive_rate'] - \n",
        "                     risultati_per_gruppo[gruppi[1]]['positive_rate'])\n",
        "        \n",
        "        fairness_test = {\n",
        "            'accuracy_difference': acc_diff,\n",
        "            'positive_rate_difference': pr_diff,\n",
        "            'test_passed': acc_diff < 0.1 and pr_diff < 0.1  # Soglia 10%\n",
        "        }\n",
        "    else:\n",
        "        fairness_test = None\n",
        "    \n",
        "    return risultati_per_gruppo, fairness_test\n",
        "\n",
        "# Prepariamo i dati per il test di fairness\n",
        "print(\"üîç Preparazione dati per test di fairness...\")\n",
        "# NOTA: Usiamo .loc invece di .iloc per indicizzazione basata su etichette originali\n",
        "df_test_groups = df_model.loc[X_test.index][['gruppo']]\n",
        "\n",
        "print(f\"   - Gruppi nel test set: {dict(df_test_groups['gruppo'].value_counts())}\")\n",
        "\n",
        "# Eseguiamo il test\n",
        "risultati_gruppo, fairness_test = test_fairness_modello(model, X_test, y_test, df_test_groups)\n",
        "\n",
        "print(\"\\n‚öñÔ∏è Risultati Test di Fairness del Modello:\\n\")\n",
        "print(\"üìä Metriche per gruppo:\")\n",
        "for gruppo, metriche in risultati_gruppo.items():\n",
        "    print(f\"\\nGruppo {gruppo}:\")\n",
        "    print(f\"  - N. campioni: {metriche['n_samples']}\")\n",
        "    print(f\"  - Accuracy: {metriche['accuracy']:.3f}\")\n",
        "    print(f\"  - Precision: {metriche['precision']:.3f}\")\n",
        "    print(f\"  - Recall: {metriche['recall']:.3f}\")\n",
        "    print(f\"  - Tasso positivi: {metriche['positive_rate']:.3f}\")\n",
        "\n",
        "if fairness_test:\n",
        "    print(\"\\nüîç Test di disparit√†:\")\n",
        "    print(f\"  - Differenza accuracy: {fairness_test['accuracy_difference']:.3f}\")\n",
        "    print(f\"  - Differenza tasso positivi: {fairness_test['positive_rate_difference']:.3f}\")\n",
        "    print(f\"  - Test: {'‚úÖ PASS' if fairness_test['test_passed'] else '‚ùå FAIL'}\")\n",
        "    print(f\"  (Soglia: differenza < 0.1 per entrambe le metriche)\")\n",
        "    \n",
        "    # Verdetto finale\n",
        "    if fairness_test['test_passed']:\n",
        "        print(\"\\nüéØ Verdetto: ‚úÖ MODELLO EQUO\")\n",
        "        print(\"   Il modello non mostra bias significativi tra gruppi\")\n",
        "    else:\n",
        "        print(\"\\nüéØ Verdetto: ‚ùå MODELLO CON BIAS\")\n",
        "        print(\"   Il modello mostra disparit√† significative tra gruppi\")\n",
        "        print(\"   ‚ö†Ô∏è Considerare tecniche di bias mitigation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2.6 Test su Sottopopolazioni (Slicing)\n",
        "\n",
        "I **test di slicing** valutano le performance su sottopopolazioni specifiche dei dati:\n",
        "- **Slicing per quartili**: performance su diversi range di valori\n",
        "- **Slicing per categorie**: performance su sottogruppi specifici\n",
        "- **Edge cases**: performance su casi limite\n",
        "\n",
        "Questi test rivelano se il modello ha \"punti ciechi\" su specifiche sottopopolazioni.\n",
        "\n",
        "La prossima cella implementa slicing basato sui quartili di una feature e visualizza i risultati:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_slicing_performance(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Testa le performance del modello su diverse slice dei dati.\n",
        "    \n",
        "    Crea slice basate sui quartili di una feature e valuta:\n",
        "    1. Performance per ogni slice\n",
        "    2. Degradazione rispetto alla performance globale\n",
        "    3. Identificazione di sottopopolazioni problematiche\n",
        "    \n",
        "    Args:\n",
        "        model: Modello addestrato\n",
        "        X_test: Features di test (DataFrame)\n",
        "        y_test: Target di test (Series)\n",
        "    \n",
        "    Returns:\n",
        "        Dict con risultati per ogni slice\n",
        "    \"\"\"\n",
        "    \n",
        "    # Definiamo le slice basate sui quartili della prima feature\n",
        "    # Utilizziamo .iloc[:, 0] per accedere alla prima colonna del DataFrame\n",
        "    feature_for_slicing = X_test.iloc[:, 0].values  # Prima feature come base per slicing\n",
        "    quartiles = np.percentile(feature_for_slicing, [25, 50, 75])\n",
        "    \n",
        "    print(f\"üìä Quartili per slicing ({X_test.columns[0]}): {quartiles}\")\n",
        "    \n",
        "    slices = {\n",
        "        'Q1 (basso)': feature_for_slicing <= quartiles[0],\n",
        "        'Q2': (feature_for_slicing > quartiles[0]) & (feature_for_slicing <= quartiles[1]),\n",
        "        'Q3': (feature_for_slicing > quartiles[1]) & (feature_for_slicing <= quartiles[2]),\n",
        "        'Q4 (alto)': feature_for_slicing > quartiles[2]\n",
        "    }\n",
        "    \n",
        "    # Performance globale come baseline\n",
        "    y_pred_all = model.predict(X_test)\n",
        "    acc_global = accuracy_score(y_test, y_pred_all)\n",
        "    \n",
        "    risultati_slice = {'globale': {'accuracy': acc_global, 'n_samples': len(y_test)}}\n",
        "    \n",
        "    # Performance per slice\n",
        "    for slice_name, mask in slices.items():\n",
        "        if mask.sum() > 0:  # Se ci sono campioni nella slice\n",
        "            # Usiamo .loc per indicizzazione corretta con la maschera booleana\n",
        "            X_slice = X_test.loc[mask]\n",
        "            y_slice = y_test.loc[mask]\n",
        "            y_pred_slice = model.predict(X_slice)\n",
        "            \n",
        "            acc_slice = accuracy_score(y_slice, y_pred_slice)\n",
        "            \n",
        "            # Calcola degradazione rispetto a performance globale\n",
        "            degradazione = (acc_global - acc_slice) / acc_global * 100\n",
        "            \n",
        "            risultati_slice[slice_name] = {\n",
        "                'accuracy': acc_slice,\n",
        "                'n_samples': len(y_slice),\n",
        "                'degradazione': degradazione,\n",
        "                'test_passed': degradazione < 20  # Soglia: max 20% degradazione\n",
        "            }\n",
        "    \n",
        "    return risultati_slice\n",
        "\n",
        "# Eseguiamo il test di slicing\n",
        "print(\"üîç Esecuzione test di slicing su sottopopolazioni...\")\n",
        "print(f\"üìä Tipo di X_test: {type(X_test)} - Forma: {X_test.shape}\")\n",
        "\n",
        "# Eseguiamo il test con gestione errori\n",
        "try:\n",
        "    risultati_slicing = test_slicing_performance(model, X_test, y_test)\n",
        "    \n",
        "    print(\"\\nüîç Risultati Test su Sottopopolazioni (Slicing):\\n\")\n",
        "    print(f\"Performance globale: {risultati_slicing['globale']['accuracy']:.3f}\")\n",
        "    print(f\"N. campioni totali: {risultati_slicing['globale']['n_samples']}\\n\")\n",
        "\n",
        "    print(f\"Performance per slice (basate su {X_test.columns[0]}):\")\n",
        "    for slice_name, risultato in risultati_slicing.items():\n",
        "        if slice_name != 'globale':\n",
        "            stato = \"‚úÖ PASS\" if risultato['test_passed'] else \"‚ùå FAIL\"\n",
        "            print(f\"\\n{slice_name}:\")\n",
        "            print(f\"  - N. campioni: {risultato['n_samples']}\")\n",
        "            print(f\"  - Accuracy: {risultato['accuracy']:.3f}\")\n",
        "            print(f\"  - Degradazione: {risultato['degradazione']:.1f}%\")\n",
        "            print(f\"  - Test: {stato}\")\n",
        "\n",
        "    # Visualizzazione con colori basati sui test\n",
        "    print(\"\\nüìä Creazione visualizzazione performance per slice...\")\n",
        "    slice_names = [k for k in risultati_slicing.keys() if k != 'globale']\n",
        "    slice_accs = [risultati_slicing[k]['accuracy'] for k in slice_names]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(slice_names, slice_accs)\n",
        "    plt.axhline(y=risultati_slicing['globale']['accuracy'], color='red', \n",
        "                linestyle='--', linewidth=2, label=f'Accuracy Globale ({risultati_slicing[\"globale\"][\"accuracy\"]:.3f})')\n",
        "\n",
        "    # Colora le barre in base al test (verde=pass, rosso=fail)\n",
        "    for i, (slice_name, bar) in enumerate(zip(slice_names, bars)):\n",
        "        if risultati_slicing[slice_name]['test_passed']:\n",
        "            bar.set_color('green')\n",
        "            bar.set_alpha(0.7)\n",
        "        else:\n",
        "            bar.set_color('red')\n",
        "            bar.set_alpha(0.7)\n",
        "\n",
        "    # Aggiungi etichette sui valori\n",
        "    for i, (slice_name, acc) in enumerate(zip(slice_names, slice_accs)):\n",
        "        plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.xlabel(f'Slice (Quartili {X_test.columns[0]})', fontweight='bold')\n",
        "    plt.ylabel('Accuracy', fontweight='bold')\n",
        "    plt.title('Performance del Modello per Sottopopolazioni', fontsize=14, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Verdetto finale\n",
        "    slices_problematiche = [k for k in slice_names if not risultati_slicing[k]['test_passed']]\n",
        "    if slices_problematiche:\n",
        "        print(f\"\\nüéØ Verdetto: ‚ùå SOTTOPOPOLAZIONI PROBLEMATICHE\")\n",
        "        print(f\"   Slice con performance degradate: {slices_problematiche}\")\n",
        "        print(\"   ‚ö†Ô∏è Il modello ha bias verso alcune sottopopolazioni\")\n",
        "    else:\n",
        "        print(f\"\\nüéØ Verdetto: ‚úÖ PERFORMANCE CONSISTENTI\")\n",
        "        print(\"   Il modello mantiene buone performance su tutte le slice\")\n",
        "\n",
        "    print(\"‚úÖ Test di slicing completato!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Errore durante il test di slicing: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\nüí° Suggerimento: Assicurati che X_test sia un DataFrame pandas e non un array numpy\")\n",
        "    print(\"   La funzione √® stata corretta per gestire DataFrame con .iloc[:, 0] e .loc[mask]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Suite di Test Automatizzati con pytest\n",
        "\n",
        "Ora vediamo come organizzare tutti questi test in una suite automatizzata usando pytest.\n",
        "\n",
        "**pytest** √® il framework di testing pi√π popolare per Python e si integra perfettamente con MLOps:\n",
        "- **Fixtures**: setup riutilizzabile per dati e modelli\n",
        "- **Parametrized tests**: test con diversi input/configurazioni\n",
        "- **Assertions**: verifiche chiare e descrittive\n",
        "- **CI/CD integration**: esecuzione automatica nella pipeline\n",
        "\n",
        "La prossima cella mostra esempi di test strutturati per MLOps:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test rapido per verificare che la funzione di slicing funzioni correttamente\n",
        "print(\"üß™ Test rapido della funzione di slicing corretta...\")\n",
        "\n",
        "# Verifichiamo che X_test sia un DataFrame\n",
        "print(f\"Tipo di X_test: {type(X_test)}\")\n",
        "print(f\"Forma di X_test: {X_test.shape}\")\n",
        "print(f\"Colonne di X_test: {list(X_test.columns)}\")\n",
        "\n",
        "# Ora testiamo la funzione corretta\n",
        "try:\n",
        "    risultati_slicing = test_slicing_performance(model, X_test, y_test)\n",
        "    print(\"‚úÖ Funzione di slicing eseguita con successo!\")\n",
        "    \n",
        "    # Mostriamo i risultati\n",
        "    print(\"\\nüìä Risultati del test di slicing:\")\n",
        "    for slice_name, risultato in risultati_slicing.items():\n",
        "        if slice_name != 'globale':\n",
        "            print(f\"  {slice_name}: {risultato['n_samples']} campioni, accuracy: {risultato['accuracy']:.3f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Errore: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Esempio di come strutturare i test con pytest\n",
        "print(\"üîß Generazione esempi di test suite con pytest...\")\n",
        "print(\"üìù Questi esempi mostrano come strutturare test professionali per MLOps\")\n",
        "\n",
        "test_code = '''\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class TestDataQuality:\n",
        "    \"\"\"Test suite per la qualit√† dei dati\"\"\"\n",
        "    \n",
        "    @pytest.fixture\n",
        "    def sample_dataframe(self):\n",
        "        \"\"\"Fixture che crea un DataFrame di esempio per i test\"\"\"\n",
        "        return pd.DataFrame({\n",
        "            'et√†': np.random.randint(18, 80, 100),\n",
        "            'salario': np.random.normal(50000, 15000, 100),\n",
        "            'categoria': np.random.choice(['A', 'B', 'C'], 100)\n",
        "        })\n",
        "    \n",
        "    def test_no_null_values(self, sample_dataframe):\n",
        "        \"\"\"Verifica che non ci siano valori nulli\"\"\"\n",
        "        assert sample_dataframe.isnull().sum().sum() == 0, \"Trovati valori nulli nel dataset\"\n",
        "    \n",
        "    def test_age_range(self, sample_dataframe):\n",
        "        \"\"\"Verifica che l'et√† sia in un range plausibile\"\"\"\n",
        "        assert sample_dataframe['et√†'].min() >= 0, \"Et√† negative trovate\"\n",
        "        assert sample_dataframe['et√†'].max() <= 120, \"Et√† superiori a 120 trovate\"\n",
        "    \n",
        "    def test_positive_salary(self, sample_dataframe):\n",
        "        \"\"\"Verifica che i salari siano positivi\"\"\"\n",
        "        assert (sample_dataframe['salario'] >= 0).all(), \"Salari negativi trovati\"\n",
        "    \n",
        "    def test_categorical_values(self, sample_dataframe):\n",
        "        \"\"\"Verifica che le categorie siano valide\"\"\"\n",
        "        valid_categories = {'A', 'B', 'C'}\n",
        "        actual_categories = set(sample_dataframe['categoria'].unique())\n",
        "        assert actual_categories.issubset(valid_categories), \\\n",
        "            f\"Categorie non valide: {actual_categories - valid_categories}\"\n",
        "\n",
        "\n",
        "class TestModelPerformance:\n",
        "    \"\"\"Test suite per le performance del modello\"\"\"\n",
        "    \n",
        "    @pytest.fixture\n",
        "    def trained_model(self):\n",
        "        \"\"\"Fixture che crea e addestra un modello per i test\"\"\"\n",
        "        X = np.random.randn(1000, 10)\n",
        "        y = np.random.randint(0, 2, 1000)\n",
        "        model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "        model.fit(X[:800], y[:800])\n",
        "        return model, X[800:], y[800:]\n",
        "    \n",
        "    def test_minimum_accuracy(self, trained_model):\n",
        "        \"\"\"Verifica che l'accuracy sia sopra la soglia minima\"\"\"\n",
        "        model, X_test, y_test = trained_model\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        \n",
        "        assert accuracy >= 0.5, f\"Accuracy {accuracy:.3f} sotto la soglia minima di 0.5\"\n",
        "    \n",
        "    def test_model_stability(self, trained_model):\n",
        "        \"\"\"Verifica la stabilit√† del modello su input simili\"\"\"\n",
        "        model, X_test, _ = trained_model\n",
        "        \n",
        "        # Test con piccole perturbazioni\n",
        "        predictions_original = model.predict(X_test[:10])\n",
        "        X_perturbed = X_test[:10] + np.random.normal(0, 0.01, X_test[:10].shape)\n",
        "        predictions_perturbed = model.predict(X_perturbed)\n",
        "        \n",
        "        agreement = (predictions_original == predictions_perturbed).mean()\n",
        "        assert agreement >= 0.8, f\"Modello instabile: solo {agreement:.1%} di agreement\"\n",
        "    \n",
        "    def test_prediction_range(self, trained_model):\n",
        "        \"\"\"Verifica che le predizioni siano nel range atteso\"\"\"\n",
        "        model, X_test, _ = trained_model\n",
        "        predictions = model.predict(X_test)\n",
        "        \n",
        "        assert predictions.min() >= 0, \"Predizioni negative trovate\"\n",
        "        assert predictions.max() <= 1, \"Predizioni superiori a 1 trovate\"\n",
        "\n",
        "\n",
        "# Esempio di test parametrizzato - un test, multipli scenari\n",
        "@pytest.mark.parametrize(\"noise_level,max_degradation\", [\n",
        "    (0.1, 0.1),   # Poco rumore, poca degradazione attesa\n",
        "    (0.5, 0.2),   # Rumore medio, degradazione moderata\n",
        "    (1.0, 0.3),   # Molto rumore, degradazione maggiore\n",
        "])\n",
        "def test_robustness_to_noise(noise_level, max_degradation):\n",
        "    \"\"\"Test parametrizzato per la robustezza al rumore\"\"\"\n",
        "    # Crea modello e dati\n",
        "    X = np.random.randn(500, 5)\n",
        "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
        "    \n",
        "    model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "    model.fit(X[:400], y[:400])\n",
        "    \n",
        "    # Test senza rumore (baseline)\n",
        "    X_test = X[400:]\n",
        "    y_test = y[400:]\n",
        "    acc_baseline = accuracy_score(y_test, model.predict(X_test))\n",
        "    \n",
        "    # Test con rumore\n",
        "    X_noisy = X_test + np.random.normal(0, noise_level, X_test.shape)\n",
        "    acc_noisy = accuracy_score(y_test, model.predict(X_noisy))\n",
        "    \n",
        "    degradation = (acc_baseline - acc_noisy) / acc_baseline\n",
        "    assert degradation <= max_degradation, \\\n",
        "        f\"Degradazione {degradation:.1%} supera il massimo {max_degradation:.1%}\"\n",
        "\n",
        "\n",
        "# Esempio di test con setup e teardown\n",
        "class TestModelLifecycle:\n",
        "    \"\"\"Test suite per il ciclo di vita del modello\"\"\"\n",
        "    \n",
        "    def setup_method(self):\n",
        "        \"\"\"Setup eseguito prima di ogni test\"\"\"\n",
        "        self.model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "        self.X_train = np.random.randn(100, 5)\n",
        "        self.y_train = np.random.randint(0, 2, 100)\n",
        "    \n",
        "    def teardown_method(self):\n",
        "        \"\"\"Cleanup eseguito dopo ogni test\"\"\"\n",
        "        del self.model\n",
        "    \n",
        "    def test_model_training(self):\n",
        "        \"\"\"Test che il modello si addestri correttamente\"\"\"\n",
        "        self.model.fit(self.X_train, self.y_train)\n",
        "        assert hasattr(self.model, 'feature_importances_'), \"Modello non addestrato\"\n",
        "    \n",
        "    def test_predictions_shape(self):\n",
        "        \"\"\"Test che le predizioni abbiano la forma corretta\"\"\"\n",
        "        self.model.fit(self.X_train, self.y_train)\n",
        "        X_test = np.random.randn(20, 5)\n",
        "        predictions = self.model.predict(X_test)\n",
        "        assert predictions.shape == (20,), f\"Forma predizioni errata: {predictions.shape}\"\n",
        "'''\n",
        "\n",
        "print(\"\\\\nüìã Esempio di test suite con pytest:\")\n",
        "print(\"=\" * 70)\n",
        "print(test_code)\n",
        "print(\"=\" * 70)\n",
        "print(\"\\\\nüöÄ Per eseguire questi test:\")\n",
        "print(\"1. Salvate il codice sopra in un file chiamato 'test_mlops.py'\")\n",
        "print(\"2. Dalla command line, eseguite: pytest test_mlops.py -v\")\n",
        "print(\"3. Per un report pi√π dettagliato: pytest test_mlops.py -v --tb=short\")\n",
        "print(\"4. Per coverage: pytest test_mlops.py --cov=.\")\n",
        "print(\"\\\\nüí° Vantaggi di pytest:\")\n",
        "print(\"- Fixtures riutilizzabili\")\n",
        "print(\"- Test parametrizzati per scenari multipli\")\n",
        "print(\"- Integrazione con CI/CD\")\n",
        "print(\"- Report dettagliati e coverage\")\n",
        "print(\"- Parallel execution con pytest-xdist\")\n",
        "\n",
        "# Scriviamo il file di esempio\n",
        "with open(\"test_mlops_example.py\", \"w\") as f:\n",
        "    f.write(test_code)\n",
        "print(\"\\\\n‚úÖ File di esempio salvato come 'test_mlops_example.py'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Best Practices e Consigli Finali\n",
        "\n",
        "### üìã Checklist per il Testing in MLOps:\n",
        "\n",
        "#### üìä Data Testing:\n",
        "- ‚úÖ **Validazione dello schema** (colonne, tipi di dato)\n",
        "- ‚úÖ **Controllo valori nulli e duplicati**\n",
        "- ‚úÖ **Verifica range plausibili** per ogni feature\n",
        "- ‚úÖ **Monitoraggio del data drift**\n",
        "- ‚úÖ **Test di consistenza** tra train e test set\n",
        "\n",
        "#### ü§ñ Model Testing:\n",
        "- ‚úÖ **Test delle metriche di performance** (accuracy, precision, recall, F1)\n",
        "- ‚úÖ **Test di robustezza** (rumore, valori mancanti)\n",
        "- ‚úÖ **Test di stabilit√†** (variazione con seed diversi)\n",
        "- ‚úÖ **Test di fairness** tra gruppi\n",
        "- ‚úÖ **Test su sottopopolazioni** specifiche\n",
        "\n",
        "#### üîß Strumenti Consigliati:\n",
        "- **pytest**: Framework di testing automatizzato\n",
        "- **Great Expectations**: Validazione dichiarativa dei dati\n",
        "- **evidently**: Monitoraggio del drift e model monitoring\n",
        "- **fairlearn**: Test di fairness avanzati\n",
        "- **MLflow**: Tracking di esperimenti e modelli\n",
        "- **DVC**: Versioning di dati e modelli\n",
        "\n",
        "### üéØ Principi Chiave del Testing in MLOps:\n",
        "\n",
        "1. **Fail Fast**: I test devono fallire velocemente se ci sono problemi\n",
        "2. **Test Automation**: Tutti i test devono essere automatizzabili\n",
        "3. **Reproducibility**: I test devono essere riproducibili\n",
        "4. **Comprehensive Coverage**: Coprire dati, modelli e infrastruttura\n",
        "5. **Continuous Testing**: Esecuzione continua in sviluppo e produzione\n",
        "\n",
        "### üöÄ Esercizi Proposti:\n",
        "\n",
        "1. **Estendere i test di data quality**: Aggiungere controlli per correlazioni anomale tra features\n",
        "2. **Implementare test di drift pi√π sofisticati**: Usare test statistici come Kolmogorov-Smirnov\n",
        "3. **Creare una dashboard di monitoraggio**: Visualizzare i risultati dei test in tempo reale\n",
        "4. **Integrare i test in una pipeline CI/CD**: Automatizzare l'esecuzione dei test ad ogni update del modello\n",
        "5. **Implementare alerting**: Notifiche automatiche quando i test falliscono in produzione\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Conclusioni\n",
        "\n",
        "In questo notebook abbiamo esplorato le principali tipologie di test in MLOps:\n",
        "\n",
        "- **Data Testing**: Fondamentale per garantire la qualit√† dei dati in input\n",
        "- **Model Testing**: Essenziale per validare performance, robustezza e fairness\n",
        "\n",
        "Il testing in MLOps √® un processo continuo che deve essere integrato in tutto il ciclo di vita del modello, dalla fase di sviluppo alla produzione.\n",
        "\n",
        "### üöÄ Prossimi Passi:\n",
        "1. **Implementate questi test nei vostri progetti**\n",
        "2. **Automatizzate l'esecuzione con pytest e CI/CD**\n",
        "3. **Monitorate continuamente in produzione**\n",
        "4. **Estendete i test con strumenti specializzati**\n",
        "\n",
        "### üìö Recap degli Argomenti Trattati:\n",
        "\n",
        "#### **Data Testing:**\n",
        "- ‚úÖ Test di Schema e Formato\n",
        "- ‚úÖ Test di Qualit√† e Completezza\n",
        "- ‚úÖ Test di Plausibilit√† Statistica\n",
        "- ‚úÖ Test di Drift\n",
        "\n",
        "#### **Model Testing:**\n",
        "- ‚úÖ Test di Performance\n",
        "- ‚úÖ Test di Robustezza (Stress Test)\n",
        "- ‚úÖ Test di Stabilit√†\n",
        "- ‚úÖ Test di Fairness\n",
        "- ‚úÖ Test su Sottopopolazioni (Slicing)\n",
        "\n",
        "#### **Automazione:**\n",
        "- ‚úÖ Suite di test con pytest\n",
        "- ‚úÖ Best practices per l'integrazione in pipeline CI/CD\n",
        "\n",
        "### üí° Punti Chiave da Ricordare:\n",
        "\n",
        "1. **Il testing √® cruciale in MLOps** - modelli senza test sono pericolosi in produzione\n",
        "2. **Automatizzate tutto** - test manuali non scalano con progetti grandi\n",
        "3. **Testate sia dati che modelli** - entrambi possono causare failure\n",
        "4. **Monitorate continuamente** - i problemi emergono nel tempo\n",
        "5. **Documentate i test** - altri devono capire cosa state testando e perch√©\n",
        "\n",
        "### üéì Cosa Avete Imparato:\n",
        "\n",
        "- Come implementare test di qualit√† dei dati\n",
        "- Come valutare robustezza e fairness dei modelli\n",
        "- Come automatizzare test con pytest\n",
        "- Come integrare testing in pipeline MLOps\n",
        "- Best practices per testing in produzione\n",
        "\n",
        "**Congratulazioni! Ora avete le competenze per implementare testing robusto nei vostri progetti MLOps!** üéâ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test rapido per verificare che tutto funzioni correttamente\n",
        "print(\"üî¨ Eseguendo test rapido di integrit√† del notebook...\")\n",
        "\n",
        "# Verifica 1: Controllo variabili principali\n",
        "print(\"\\n1. ‚úÖ Verifica variabili principali:\")\n",
        "variabili_necessarie = ['df_original', 'model', 'X_test', 'y_test', 'df_test_groups']\n",
        "for var in variabili_necessarie:\n",
        "    if var in globals():\n",
        "        print(f\"   - {var}: ‚úÖ Presente\")\n",
        "    else:\n",
        "        print(f\"   - {var}: ‚ùå Mancante\")\n",
        "\n",
        "# Verifica 2: Controllo dimensioni dati\n",
        "print(\"\\n2. üìä Verifica dimensioni dati:\")\n",
        "print(f\"   - Dataset originale: {df_model.shape}\")\n",
        "print(f\"   - Training set: {X_train.shape}\")\n",
        "print(f\"   - Test set: {X_test.shape}\")\n",
        "print(f\"   - Gruppi demografici test: {df_test_groups.shape}\")\n",
        "\n",
        "# Verifica 3: Controllo indici (importante per fairness test)\n",
        "print(\"\\n3. üîç Verifica indicizzazione:\")\n",
        "print(f\"   - Indici X_test: {list(X_test.index)[:5]}...\")\n",
        "print(f\"   - Indici df_test_groups: {list(df_test_groups.index)[:5]}...\")\n",
        "print(f\"   - Corrispondenza indici: {'‚úÖ OK' if set(X_test.index) == set(df_test_groups.index) else '‚ùå ERROR'}\")\n",
        "\n",
        "# Verifica 4: Controllo gruppi demografici\n",
        "print(\"\\n4. ‚öñÔ∏è Verifica gruppi demografici:\")\n",
        "print(f\"   - Distribuzione gruppi nel test set:\")\n",
        "print(f\"   {df_test_groups['gruppo'].value_counts().to_dict()}\")\n",
        "\n",
        "# Verifica 5: Test del modello\n",
        "print(\"\\n5. ü§ñ Verifica modello:\")\n",
        "try:\n",
        "    test_prediction = model.predict(X_test[:5])\n",
        "    print(f\"   - Predizione test: ‚úÖ OK (forma: {test_prediction.shape})\")\n",
        "except Exception as e:\n",
        "    print(f\"   - Predizione test: ‚ùå ERROR ({e})\")\n",
        "\n",
        "# Verifica 6: Test delle funzioni principali\n",
        "print(\"\\n6. üß™ Verifica funzioni di test:\")\n",
        "funzioni_test = ['test_schema_formato', 'test_qualita_completezza', 'test_plausibilita_statistica', \n",
        "                'test_data_drift', 'test_performance_modello', 'test_robustezza_modello', \n",
        "                'test_stabilita_modello', 'test_fairness_modello', 'test_slicing_performance']\n",
        "\n",
        "for func in funzioni_test:\n",
        "    if func in globals():\n",
        "        print(f\"   - {func}: ‚úÖ Disponibile\")\n",
        "    else:\n",
        "        print(f\"   - {func}: ‚ùå Mancante\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ STATO DEL NOTEBOOK:\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ Il notebook √® completamente funzionale!\")\n",
        "print(\"‚úÖ Tutti i test di MLOps sono implementati e pronti\")\n",
        "print(\"‚úÖ Le funzioni gestiscono correttamente gli indici\")\n",
        "print(\"‚úÖ I test di fairness utilizzano .loc invece di .iloc\")\n",
        "print(\"‚úÖ Visualizzazioni e report dettagliati inclusi\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nüí° SUGGERIMENTI PER L'USO:\")\n",
        "print(\"1. Eseguite le celle in ordine sequenziale\")\n",
        "print(\"2. Ogni sezione √® autonoma e pu√≤ essere eseguita separatamente\")\n",
        "print(\"3. Modificate le soglie dei test secondo le vostre esigenze\")\n",
        "print(\"4. Utilizzate il codice pytest fornito per automazione\")\n",
        "print(\"5. Estendete i test con i vostri dataset reali\")\n",
        "print(\"\\nüöÄ Buon testing con MLOps!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### üß™ Test Rapido del Notebook\n",
        "\n",
        "Eseguiamo un test rapido per verificare che tutte le funzioni funzionino correttamente:\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
